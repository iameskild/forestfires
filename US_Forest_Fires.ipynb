{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Forest Fire Data\n",
    "Dataset includes 24 years of US forest fires geospatial data from 1992-2015. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at the Big Picture\n",
    "\n",
    "As someone passionate about trees and the forests they help create, I am interested in examining the causes, size and intensity of forest fires in the US. Given the US Forest Fires dataset found on kaggle.com, I spend some time here exploring and visualizing the data. There are a lot of other interesting analyses that I might come back to but this is a solid start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the Data\n",
    "\n",
    "Data was downloaded as a sqlite database from Kaggle.com.\n",
    "\n",
    "[Link to dataset](https://www.kaggle.com/rtatman/188-million-us-wildfires)\n",
    "\n",
    "### Dataset Description from Kaggle.com\n",
    "\"This data publication contains a spatial database of wildfires that occurred in the United States from 1992 to 2015. It is the third update of a publication originally generated to support the national Fire Program Analysis (FPA) system. The wildfire records were acquired from the reporting systems of federal, state, and local fire organizations. The following core data elements were required for records to be included in this data publication: discovery date, final fire size, and a point location at least as precise as Public Land Survey System (PLSS) section (1-square mile grid). The data were transformed to conform, when possible, to the data standards of the National Wildfire Coordinating Group (NWCG). Basic error-checking was performed and redundant records were identified and removed, to the degree possible. The resulting product, referred to as the Fire Program Analysis fire-occurrence database (FPA FOD), includes 1.88 million geo-referenced wildfire records, representing a total of 140 million acres burned during the 24-year period.\"\n",
    "\n",
    "### Acknowledgements:\n",
    "These data were collected using funding from the U.S. Government and can be used without additional permissions or fees. If you use these data in a publication, presentation, or other research product please use the following citation:\n",
    "\n",
    "Short, Karen C. 2017. Spatial wildfire occurrence data for the United States, 1992-2015 [FPAFOD20170508]. 4th Edition. Fort Collins, CO: Forest Service Research Data Archive. https://doi.org/10.2737/RDS-2013-0009.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import calplot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "type(0x7A6CF84A)  # %matplotli 0x7A6CF84A\n",
    "\n",
    "import plotly.express as px\n",
    "from plotly import offline\n",
    "from plotly.offline import init_notebook_mode\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import datetime as dt\n",
    "import json\n",
    "import sqlite3\n",
    "from os import path\n",
    "\n",
    "import requests\n",
    "import topojson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables and settings\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.rcParams.update({\"figure.figsize\": (15, 10)})\n",
    "\n",
    "seq_cmap = \"YlOrRd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_connection(db):\n",
    "    \"\"\" Create a sqlite3 database connection. \"\"\"\n",
    "\n",
    "    # check sqlite3 database file exists\n",
    "    try:\n",
    "        path.exists(db)\n",
    "    except:\n",
    "        print(\"sqlite3 database file does not exist.\")\n",
    "        return False\n",
    "\n",
    "    # connect to the sqlite3 database\n",
    "    conn = None\n",
    "    try:\n",
    "\n",
    "        conn = sqlite3.connect(db)\n",
    "        cur = conn.cursor()\n",
    "        print(\"sqlite3 version:\", sqlite3.version)\n",
    "        return conn, cur\n",
    "    except:\n",
    "        return sqlite3.Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn, cur = sqlite3_connection(\"FPA_FOD_20170508.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_execute(q, cur=cur, fetchall=True):\n",
    "    \"\"\" Securely execute specific sqlite3. \"\"\"\n",
    "\n",
    "    try:\n",
    "        cur.execute(q)\n",
    "        if fetchall:\n",
    "            return cur.fetchall()\n",
    "        else:\n",
    "            return cur.fetchone()\n",
    "    except:\n",
    "        return sqlite3.Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the database tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = sqlite3_execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The number of tables in the database: {len(tables)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    SELECT * FROM Fires\n",
    "    LIMIT 10;\n",
    "    \"\"\"\n",
    "\n",
    "sqlite3_execute(q, fetchall=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Fires` table appears to be the data we are looking for. The next question is, what do each of the other tables represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "print(f\"{'TABLE' : <30}{'NUMBER OF ROWS' : >10}\")\n",
    "\n",
    "for t in tables:\n",
    "    q = f\"\"\"\n",
    "        SELECT COUNT(*) FROM {t[0]};\n",
    "        \"\"\"\n",
    "\n",
    "    try:\n",
    "        shape = sqlite3_execute(q)\n",
    "        nrows = shape[0][0]\n",
    "        if nrows != 0:\n",
    "            print(f\"{f'{t[0]}':<30}{f'{nrows}':<10}\")\n",
    "            count += 1\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "print(f\"\\nThe number of non-empty tables: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just about half of the tables in this sqlite database are empty or raise `sqlite3.Error` messages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"\"\"\n",
    "    SELECT * FROM NWCG_UnitIDActive_20170109;\n",
    "    \"\"\"\n",
    "\n",
    "pd.read_sql(q, conn).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By simplying looking at the tables themselves (using `pd.read_sql`), the following tables seem to be of interest and may need to be merged in some fashion:\n",
    "* `Fires` \n",
    "* `idx_Fires_Shape`\n",
    "* `idx_Fires_Shape_node`\n",
    "* `idx_Fires_Shape_rowid`\n",
    "* `idx_Fires_Shape_parent`\n",
    "* `NWCG_UnitIDActive_20170109`\n",
    "    * \"Look-up table containing all NWCG identifiers for agency units that were active (i.e., valid) as of 9 January 2017, when the list was downloaded from https://www.nifc.blm.gov/unit_id/Publish.html and used as the source of values available to populate the following fields in the Fires table: NWCGREPORTINGAGENCY, NWCGREPORTINGUNITID, and NWCGREPORTINGUNITNAME.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the `Fires` table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "q = \"\"\"\n",
    "    SELECT * FROM Fires;\n",
    "    \"\"\"\n",
    "\n",
    "# `fires` is a copy of the `Fires` table\n",
    "fires = pd.read_sql(q, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save `fires` table to a csv\n",
    "fires.to_csv(\"fires.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.NWCG_REPORTING_AGENCY.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_CODE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `low_memory` set to False ensures that there are no mixed types\n",
    "fires = pd.read_csv(\"fires.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial obeservations \n",
    "Many of the columns represent data specific to the agency in charge of fire and how those agency track the fires internally. Most of this data is of little use to us. The columns that are of most interest to us start at column 19 or `FIRE_YEAR`.\n",
    "\n",
    "To start, let's look at the discovery date (`DISCOVERY_DATE`) and containment date (`CONT_DATE`) to confirm that the date range is between 1992 and 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(fires.FIRE_YEAR.value_counts(dropna=False).sort_index().plot(kind=\"line\", x=fires))\n",
    "\n",
    "plt.title(\"Number of US Forest Fires between 1992 and 2015\")\n",
    "plt.ylabel(\"Number of total fires\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_DATE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_DATE.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_DOY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_TIME.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_TIME.isna().sum() / len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.DISCOVERY_TIME.plot(kind=\"hist\", bins=25)\n",
    "\n",
    "plt.title(\"Discovery Times for US Forest Fires\")\n",
    "plt.xlabel(\"Time (24H clock)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about when fires were \"discovered\"\n",
    "\n",
    "The `DISCOVERY_DATE` are unrecognizable. However, we are in luck, this question was already answered in the [Kaggle discussion section for this dataset](https://www.kaggle.com/rtatman/188-million-us-wildfires/discussion/39627). These appear to be Julian dates and \"dates are simply a continuous count of days and fractions since noon Universal Time on January 1, 4713 BC (on the Julian calendar)\". This makes a lot more sense. \n",
    "\n",
    "As for `DISCOVERY_DOY`, they appear to represent the day of the year, making this a redudant column.\n",
    "\n",
    "And lastly, the `DISCOVERY_TIME` appear to represent the time of the day. This distribution appears to be fairly Gaussian with a mean around 15:00 in the afternoon. However just under half of these times are missing. This begs the question, if almost half of the data are missing, is it worth including this attribute in further analysis? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.STAT_CAUSE_CODE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.STAT_CAUSE_DESCR.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about the \"statistical\" cause of the fire\n",
    "\n",
    "There are two attributes which represent the \"statistical\" cause of the fire. `STAT_CAUSE_DESCR` provides a brief description of the statistcal cause and `STAT_CAUSE_CODE` encodes these descriptions. There are no missing values for either, that said two of the largest \"statistical\" causes are \"Miscellaneous\" and \"Missing/Undefined\". \n",
    "\n",
    "This attribute represent a potential target variable, or y-variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_DATE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_DATE.isna().sum() / len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_DOY.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_train.CONT_TIME.isna().sum() / len(fires_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires[fires.CONT_DATE.isna()].FIRE_SIZE_CLASS.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE_CLASS.value_counts(dropna=False, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_DATE.isna().sum() / len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_TIME.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.CONT_TIME.plot(kind=\"hist\", bins=25)\n",
    "\n",
    "plt.title(\"Time of Containment for US Forest Fires\")\n",
    "plt.xlabel(\"Time (24H clock)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about the \"containment\" date\n",
    "\n",
    "Again, the `CONT_DATE` appears to be Julian dates but unlike `DISCOVERY_DATE`, half of the values seem to be missing. Since this attribute will be useful in determining the duration of the fires, it would be useful to impute those missing values if possible.\n",
    "\n",
    "`CONT_DOY` also seems to represent the day of the year and therefore a redudant attribute.\n",
    "\n",
    "Lastly, `CONT_TIME` attribute is missing over 50% of its values and again, the question is whether this attribute is worth using for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.STAT_CAUSE_CODE.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.STAT_CAUSE_DESCR.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE_CLASS.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE_CLASS.value_counts(dropna=False).sort_index().plot(kind=\"bar\")\n",
    "\n",
    "plt.title(\"US Forest Fire Size Classes\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Size Class\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIRE_SIZE.plot(kind=\"hist\", bins=50, logy=True)\n",
    "\n",
    "plt.title(\"US Forest Fire, Log Distribution of Sizes\")\n",
    "plt.ylabel(\"Frequency (log scale)\")\n",
    "plt.xlabel(\"Size of Fire (acres)\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about the \"size\" of the fire\n",
    "\n",
    "The `FIRE_SIZE` attribute represents the \"estimate of acres within the final perimeter of the fire\". The data is heavily skewed to the left (smaller fires). \n",
    "\n",
    "From the kaggle.com description of `FIRE_SIZE_CLASS` attribute:\n",
    "* A=greater than 0 but less than or equal to 0.25 acres\n",
    "* B=0.26-9.9 acres\n",
    "* C=10.0-99.9 acres \n",
    "* D=100-299 acres \n",
    "* E=300 to 999 acres\n",
    "* F=1000 to 4999 acres\n",
    "* G=5000+ acres\n",
    "\n",
    "There are no missing values and these attributes might be able to help us impute the containment date. If the fire is small (class A or B) perhaps that's an indication that the fire was contained the same day it was discovered.\n",
    "\n",
    "`FIRE_SIZE` is a potential target variable or y-variable for future machine learning exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.LATITUDE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.LONGITUDE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.plot(\n",
    "    kind=\"scatter\",\n",
    "    x=\"LONGITUDE\",\n",
    "    y=\"LATITUDE\",\n",
    "    alpha=0.4,\n",
    "    s=fires[\"FIRE_SIZE\"] / 500,\n",
    "    c=np.log(fires[\"FIRE_SIZE\"]),\n",
    "    figsize=(15, 10),\n",
    "    colormap=plt.get_cmap(\"spring\"),\n",
    ")\n",
    "\n",
    "plt.title(\"US Forest Fire Locations\")\n",
    "plt.ylabel(\"Latitude\")\n",
    "plt.xlabel(\"Longitude\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about the fire locations\n",
    "\n",
    "We can clearly make out the continental United States, Hawaii, Alaska and Puerto Rico simply by plotting the location (and size) of the forest fires. There appears to be a greater concentration of large fires in the western half of the continental US as well as many large fires in Alaska. There is a noticeable gap in the US midwest in areas around Ohio, Indiana, Illinois, and Iowa.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.OWNER_CODE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.OWNER_DESCR.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_by_state_series = fires.STATE.value_counts(dropna=False)\n",
    "fires_by_state = pd.DataFrame(\n",
    "    {\"state\": fires_by_state_series.index, \"total_fires\": fires_by_state_series.values}\n",
    ")\n",
    "fires_by_state.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "* [^1] - Thanks to user `mbostock` for the topoJSON data, more info found on their [GitHub](https://github.com/topojson/us-atlas).\n",
    "* [^2] - Thanks to Sean Gillies for his useful [`topojson.py` library](https://sgillies.net/2012/11/27/topojson-py.html) used to help convert topoJSON to geoJSON. [Link to topojson GitHub Repo](https://github.com/sgillies/topojson).\n",
    "* [^3] - Thanks to whoever create the brief tutorial on converting topoJSON to geoJSON data found [here](https://chart-studio.plotly.com/~empet/14397.embed).\n",
    "* [^4] - Thanks to user `mshafrir` for their \"US state in JSON\" [GitHub Gist](https://gist.github.com/mshafrir/2646763)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(url):\n",
    "    \"\"\" Given a url with raw json, download and return the json. \"\"\"\n",
    "\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Encountered an error:\\n\\n{SystemExit(e)}\")\n",
    "\n",
    "    return r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict_url = \"https://gist.githubusercontent.com/mshafrir/2646763/raw/8b0dbb93521f5d6889502305335104218454c2bf/states_hash.json\"\n",
    "topoJSON_url = \"https://cdn.jsdelivr.net/npm/us-atlas@3/counties-10m.json\"\n",
    "\n",
    "state_dict = get_json(state_dict_url)\n",
    "topoJSON = get_json(topoJSON_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topo2geo(topoJSON, level):\n",
    "    \"\"\" Convert a topoJSON object to geoJSON object for specified level `nation`, `states`, or `counties`. \"\"\"\n",
    "\n",
    "    topo_features = topoJSON[\"objects\"][f\"{level}\"][\"geometries\"]\n",
    "    scale = topoJSON[\"transform\"][\"scale\"]\n",
    "    translation = topoJSON[\"transform\"][\"translate\"]\n",
    "\n",
    "    geoJSON = dict(type=\"FeatureCollection\", features=[])\n",
    "\n",
    "    for k, tfeature in enumerate(topo_features):\n",
    "        geo_feature = dict(id=k, type=\"Feature\")\n",
    "        if level != \"nation\":\n",
    "            geo_feature[\"properties\"] = tfeature[\"properties\"]\n",
    "        geo_feature[\"geometry\"] = topojson.geometry(\n",
    "            tfeature, topoJSON[\"arcs\"], scale, translation\n",
    "        )\n",
    "        geoJSON[\"features\"].append(geo_feature)\n",
    "\n",
    "    return geoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geoJSON = topo2geo(topoJSON, \"states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab the 'id' for each present US state or territory from geoJSON data\n",
    "# unique numerical ID needed for plotly.express.choropleth\n",
    "state_list_map = {}\n",
    "for state in geoJSON[\"features\"]:\n",
    "    state_list_map[state[\"properties\"][\"name\"]] = state[\"id\"]\n",
    "\n",
    "\n",
    "# create a mapping dict for state abbreviations and their corresponding ID\n",
    "# i.e. {'CA' : 'California'} and {'California' : 21} --> {'CA' :  21}\n",
    "state_id_map = {}\n",
    "for sd in state_dict.items():\n",
    "    for slm in state_list_map.items():\n",
    "        if sd[1].lower() == slm[0].lower():\n",
    "            state_id_map[sd[0]] = slm[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_by_state[\"id\"] = fires_by_state[\"state\"].apply(lambda x: state_id_map[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_by_state.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(\n",
    "    fires_by_state,\n",
    "    locations=\"id\",\n",
    "    geojson=geoJSON,\n",
    "    color=\"total_fires\",\n",
    "    scope=\"usa\",\n",
    "    color_continuous_scale=px.colors.sequential.YlOrRd,\n",
    ")\n",
    "offline.iplot(fig, filename=\"fires_by_states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations about US states and their numer of total fires\n",
    "\n",
    "Note: The data above only represents 80% of the total fires since the remaining 20% are saved in the test data set. This map also does not include forest fires that occurred in US territories, namely Puerto Rico. \n",
    "\n",
    "Frmo the map, there are clearly a handful of places that experience many more forest fires than other. For example, California, Texas and Georgia appear to have the greatest number of fires. However as we saw above, the vast majority of fires are small (less than 10 acres). This map does not take the number of acres a single fire burned through as such, map underrepresent some states such as Alaska, or the US northwest; this is show to be the case when we plotted the size of the fire based on their longitute and latitude, see further up.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires_by_county_series = fires.COUNTY.value_counts(dropna=False)\n",
    "fires_by_county = pd.DataFrame(\n",
    "    {\n",
    "        \"county\": fires_by_county_series.index,\n",
    "        \"total_fires\": fires_by_county_series.values,\n",
    "    }\n",
    ")\n",
    "fires_by_county.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.COUNTY.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.COUNTY.isna().sum() / len(fires)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIPS_CODE.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.FIPS_NAME.value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires[fires[\"FIPS_NAME\"] == \"Washington\"].STATE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation about US counties, FIPS Codes and FIPS Names\n",
    "\n",
    "There are a total of 3006 counties across the US and from the data above, we see there are 3376 counties represented. What makes up for the difference? There is a clue at the fourth to last county listed, \"Humboldt / Pershing\". It appears there may be a handful of forest fires that have been labelled with two (or more) county namaes. There are also appears to be integer values present (\"5\") and over 36% of the data is missing for this attribute. \n",
    "\n",
    "As much as I would like to plot forest fires data at the county level (like we did for states), I suspect this will take considerable time to untangle.\n",
    "\n",
    "There are two FIPS columns, `FIPS_CODE` and `FIPS_NAME`, where FIPS stands for \"Federal Information Processing Standard\" which uniquely identifies counties or county equivalents in the US. As of 2008, they are no longer a \"standard\" though still in use to varying degrees. This makes this column especially difficult to us since 2008 happens to fall in the middle of the date range for our data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fires.Shape.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
